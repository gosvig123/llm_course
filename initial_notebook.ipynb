{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, io, os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.float16 dtype\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "context_length = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = False\n",
    "\n",
    "\n",
    "lr = 3e-4\n",
    "dropout = 0.05\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "training_iters = 100000\n",
    "eval_interval = 50\n",
    "eval_iters = 10\n",
    "compile = False\n",
    "checkpoint_dir = \"models\"\n",
    "checkpoint_fn = \"latest.pt\"\n",
    "checkpoint_load_fn = \"latest.pt\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "print(f\"Using {dtype} dtype\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using llm-train-2024-08-17-21-49-48 run name\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tcx7ttk7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c41e28ef8d4326a797196ad7360f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llm-train-2024-08-17-21-48-26</strong> at: <a href='https://wandb.ai/k-gosvig-methinks/llm-course/runs/tcx7ttk7' target=\"_blank\">https://wandb.ai/k-gosvig-methinks/llm-course/runs/tcx7ttk7</a><br/> View project at: <a href='https://wandb.ai/k-gosvig-methinks/llm-course' target=\"_blank\">https://wandb.ai/k-gosvig-methinks/llm-course</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240817_214826-tcx7ttk7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tcx7ttk7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60836601e07b4a2aa8a8c4f8ed5432a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168074999982815, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kristian/Developer/llm-course/llm_train/wandb/run-20240817_214948-fqeee2ol</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/k-gosvig-methinks/llm-course/runs/fqeee2ol' target=\"_blank\">llm-train-2024-08-17-21-49-48</a></strong> to <a href='https://wandb.ai/k-gosvig-methinks/llm-course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/k-gosvig-methinks/llm-course' target=\"_blank\">https://wandb.ai/k-gosvig-methinks/llm-course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/k-gosvig-methinks/llm-course/runs/fqeee2ol' target=\"_blank\">https://wandb.ai/k-gosvig-methinks/llm-course/runs/fqeee2ol</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True\n",
    "wandb_project = \"llm-course\"\n",
    "wandb_run_name = f\"llm-train-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "print(f\"Using {wandb_run_name} run name\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4096\n"
     ]
    }
   ],
   "source": [
    "with open(\"wiki.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=\"wiki_tokenizer.model\")\n",
    "\n",
    "vocabulary_size = sp.vocab_size()\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312, 471, 4037, 4053, 969, 36]\n",
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "print(encode(\"Hello, world!\"))\n",
    "print(decode([312, 471, 4037, 4053, 969, 36]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('encoded_data.pt'):\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    data = encode(text, dtype=torch.long)\n",
    "    torch.save(data, 'encoded_data.pt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 59211077\n",
      "Train data size: 53289969\n",
      "Val data size: 5921108\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "print(f\"Data size: {data_size}\")\n",
    "\n",
    "spl = int(data_size * 0.9)\n",
    "train_data = data[:spl]\n",
    "val_data = data[spl:]\n",
    "\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Val data size: {len(val_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    indices = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    input = torch.stack([data[i:i+context_length] for i in indices])\n",
    "    target = torch.stack([data[i+1:i+context_length+1] for i in indices])\n",
    "\n",
    "    input, target = input.to(device), target.to(device)\n",
    "    return input, target\n",
    "\n",
    "\n",
    "\n",
    "input, target = get_batch('train')\n",
    "print(input.shape)\n",
    "print(target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocabulary_size, embed_size)\n",
    "        self.positions = nn.Embedding(context_length, embed_size)\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocabulary_size, bias=BIAS)\n",
    "        self.apply(self._init_weights)  # Updated to use the new method name\n",
    "\n",
    "    def _init_weights(self, module):  # Changed method name\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input, targets=None):\n",
    "        loss = None\n",
    "        batch_size, sequence_length = input.shape  # Corrected shape access\n",
    "        emb = self.embeddings(input)  # batch_size, sequence_length, embed_size\n",
    "        positions = self.positions(\n",
    "            torch.arange(sequence_length, device=device)\n",
    "        )  # sequence_length, embed_size\n",
    "        x = emb + positions  # batch_size, sequence_length, embed_size\n",
    "        x = self.ln(x)  # batch_size, sequence_length, embed_size\n",
    "        logits = self.final_linear(x)  # batch_size, sequence_length, vocab_size\n",
    "        if targets is not None:\n",
    "            batch_size, sequence_length, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * sequence_length, vocab_size)\n",
    "            targets = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=0)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input, max_tokens=500):\n",
    "        for _ in range(max_tokens):\n",
    "            input = input[:, -context_length:]\n",
    "            logits = self(input)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input = torch.cat([input, next_token], dim=1)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_batch('train')\n",
    "model1 = GPT()\n",
    "model1 = model1.to(device)\n",
    "model1.forward(x,y)\n",
    "\n",
    "logits = model1(x,y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecode(newgen)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mgenerate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monce upon a time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_course/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[76], line 5\u001b[0m, in \u001b[0;36mgenerate_sample\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      3\u001b[0m t1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encode(\u001b[38;5;28minput\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m t1 \u001b[38;5;241m=\u001b[39m t1[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[0;32m----> 5\u001b[0m newgen \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecode(newgen)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[74], line 39\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, input, max_tokens)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[:, \u001b[38;5;241m-\u001b[39mcontext_length:]\n\u001b[1;32m     38\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     40\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
    "    t1 = t1[None, :]\n",
    "    newgen = model1.generate(t1, max_tokens=64)[0].tolist()\n",
    "    print(f\"Input: {input}\")\n",
    "    print(f\"Output: {decode(newgen)}\")\n",
    "\n",
    "\n",
    "generate_sample(\"once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  def __init__(self, n_heads):\n",
    "    super().__init__()\n",
    "    head_size = embed_size // n_heads\n",
    "    self.multihead = Multihead(n_heads, head_size)\n",
    "    self.feed_forward = FeedForward(embed_size)\n",
    "    self.ln1 = nn.LayerNorm(embed_size)\n",
    "    self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.multihead(self.ln1(x))\n",
    "    x = x + self.feed_forward(self.ln2(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forwardLayer(nn.Module):\n",
    "  def __init__(self, embed_size):\n",
    "    super().__init__()\n",
    "    self.network = nn.Sequential(\n",
    "      nn.Linear(embed_size, 6 * embed_size, bias=BIAS),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(6 * embed_size, embed_size, bias=BIAS),\n",
    "      nn.Dropout(dropout)\n",
    "      )\n",
    "  def forward(self, x):\n",
    "    return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "  def __init__(self, n_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "    self.combine = nn.Linear(n_heads * head_size, embed_size, bias=BIAS)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    # each head returns batch, sequence_length, head_size\n",
    "    x = self.combine(x)\n",
    "    self.dropout(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "    self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "    self.values = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "\n",
    "    self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    BS, SL, VS = x.shape\n",
    "    q=self.queries(x)\n",
    "    k=self.keys(x)\n",
    "    v=self.values(x)\n",
    "\n",
    "    attention_weights = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
    "    attention_weights = attention_weights.masked_fill(self.tril[:SL, :SL] == 0, float('-inf'))\n",
    "    attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "    x = attention_weights @ v\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096])\n",
      "8.38323974609375\n"
     ]
    }
   ],
   "source": [
    "# Change dtype to float32 for model and inputs\n",
    "dtype = torch.float  # Change this line\n",
    "model1 = model1.to(dtype)  # Ensure model is in float32\n",
    "# Ensure input is in long type for embedding\n",
    "x = x.to(torch.long)  # Convert input to Long\n",
    "y = y.to(torch.long)  # Convert target to Long if necessary\n",
    "\n",
    "logits, loss = model1(x, y)\n",
    "print(logits.shape)\n",
    "print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
